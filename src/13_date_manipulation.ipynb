{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/29 12:52:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+---------------+\n",
      "|id |st_dt     |end_dt  |avg_daily_sales|\n",
      "+---+----------+--------+---------------+\n",
      "|1  |2019-01-25|20190228|100            |\n",
      "|2  |2018-12-01|20200101|10             |\n",
      "|3  |2019-12-01|20200131|1              |\n",
      "+---+----------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "#.config(\"spark.executor.instances\", \"2\")\\\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder.master(\"local[*]\")\\\n",
    "        .appName('nyc_taxi')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [(1,'2019-01-25','20190228',100),\n",
    "(2,'2018-12-01','20200101',10),\n",
    "(3,'2019-12-01','20200131',1)]\n",
    "\n",
    "# Converting string type to date using Python function\n",
    "datetime.strptime('2019-01-25', '%Y-%m-%d')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', StringType()),\n",
    "    StructField('st_dt', StringType()),\n",
    "    StructField('end_dt', StringType()),\n",
    "    StructField('avg_daily_sales', IntegerType())    \n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting string column in format yyyy-mm-dd to date format from 2 different string type 'yyyy-mm-dd' and 'yyyymmdd'\n",
    "\n",
    "df2 = df.withColumn(\"st_dt\", to_date(col('st_dt'), 'yyyy-mm-dd'))\\\n",
    "        .withColumn('end_dt', to_date(col('end_dt'), 'yyyymmdd'))\n",
    "#df2.printSchema()\n",
    "#df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with date functions in Spark-SQL\n",
    "    - months_between()\n",
    "    - year()\n",
    "    - month()\n",
    "    - day()\n",
    "    - date_add()\n",
    "    - datediff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+---------------+----------+-----------------+--------+----+-----+---+---------+----------+----------+\n",
      "|id |st_dt     |end_dt  |avg_daily_sales|st_dt_date|months_between_dt|datediff|year|month|day|dayofweek|weekofyear|dayofmonth|\n",
      "+---+----------+--------+---------------+----------+-----------------+--------+----+-----+---+---------+----------+----------+\n",
      "|1  |2019-01-25|20190228|100            |2019-01-25|NULL             |NULL    |2019|1    |25 |6        |4         |25        |\n",
      "|2  |2018-12-01|20200101|10             |2018-01-01|NULL             |NULL    |2018|12   |1  |7        |48        |1         |\n",
      "|3  |2019-12-01|20200131|1              |2019-01-01|NULL             |NULL    |2019|12   |1  |1        |48        |1         |\n",
      "+---+----------+--------+---------------+----------+-----------------+--------+----+-----+---+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select *,\n",
    "to_date(st_dt, 'yyyy-mm-dd') as st_dt_date,\n",
    "months_between(end_dt, st_dt) as months_between_dt,\n",
    "datediff(end_dt, st_dt) as datediff,\n",
    "year(st_dt) as year,\n",
    "month(st_dt) as month,\n",
    "day(st_dt) as day,\n",
    "weekofyear(st_dt) as weekofyear,\n",
    "dayofweek(st_dt) as dayofweek,\n",
    "dayofmonth(st_dt) as dayofmonth,\n",
    "next_day(st_dt,\"Monday\") as next_day,\n",
    "dayofyear(st_dt) as dayofyear,\n",
    "dayofmonth(st_dt) as dayofmonth\n",
    "from df\n",
    "\"\"\"\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "Working with date functions in Pyspark dataframe functionality\n",
    "Pre-requisite : For any of the below mentioned functions to be applied, type should be date\n",
    "    - months_between()\n",
    "    - year()\n",
    "    - month()\n",
    "    - day()\n",
    "    - date_add()\n",
    "    - datediff()\n",
    "    - dayofweek(), dayofmonth(), dayofyear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+---------------+--------------+--------+----+-----+---+----------+----------+---------+----------+--------------------------+\n",
      "|id |st_dt     |end_dt    |avg_daily_sales|months_between|datediff|year|month|day|next_day  |weekofyear|dayofweek|dayofmonth|current_timestamp         |\n",
      "+---+----------+----------+---------------+--------------+--------+----+-----+---+----------+----------+---------+----------+--------------------------+\n",
      "|1  |2019-01-25|2019-01-28|100            |0.09677419    |3       |2019|1    |25 |2019-01-28|4         |6        |25        |2024-05-29 12:57:16.621077|\n",
      "|2  |2018-01-01|2020-01-01|10             |24.0          |730     |2018|1    |1  |2018-01-08|1         |2        |1         |2024-05-29 12:57:16.621077|\n",
      "|3  |2019-01-01|2020-01-31|1              |12.96774194   |395     |2019|1    |1  |2019-01-07|1         |3        |1         |2024-05-29 12:57:16.621077|\n",
      "+---+----------+----------+---------------+--------------+--------+----+-----+---+----------+----------+---------+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('months_between', months_between(col('end_dt'), col('st_dt')))\\\n",
    "    .withColumn('datediff', date_diff(col('end_dt'), col('st_dt')))\\\n",
    "    .withColumn('year', year('st_dt'))\\\n",
    "    .withColumn('month', month('st_dt'))\\\n",
    "    .withColumn('day', day('st_dt'))\\\n",
    "    .withColumn('next_day', next_day('st_dt',\"Monday\"))\\\n",
    "    .withColumn('weekofyear', weekofyear('st_dt'))\\\n",
    "    .withColumn('dayofweek', dayofweek('st_dt'))\\\n",
    "    .withColumn('dayofmonth', dayofmonth('st_dt'))\\\n",
    "    .withColumn('current_timestamp', current_timestamp())\\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+---------------+\n",
      "|id |st_dt     |end_dt  |avg_daily_sales|\n",
      "+---+----------+--------+---------------+\n",
      "|1  |2019-01-25|20190228|100            |\n",
      "|2  |2018-12-01|20200101|10             |\n",
      "|3  |2019-12-01|20200131|1              |\n",
      "+---+----------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select *\n",
    "from df\n",
    "\"\"\"\n",
    "spark.sql(query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
