{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ed0f97-19e4-4552-b4d3-3067582ef892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql.functions import spark_partition_id, col\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48aafe9d-77f9-467e-bbd3-380e89e50542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/03 18:50:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('Data Skew')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f862d2c-04c5-4c2e-9d4a-49006ce64ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keerthan/Learnings/Pyspark_Practice/Datasets/BankChurners.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Path\n",
    "file_path = Path().cwd() / 'Datasets' / 'BankChurners.csv'\n",
    "file_path = str(file_path)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f06a2e0-b320-4bdf-b668-bb74e6b654b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [CLIENTNUM#17,Attrition_Flag#18,Customer_Age#19,Gender#20,Dependent_count#21,Education_Level#22,Marital_Status#23,Income_Category#24,Card_Category#25,Months_on_book#26,Total_Relationship_Count#27,Months_Inactive_12_mon#28,Contacts_Count_12_mon#29,Credit_Limit#30,Total_Revolving_Bal#31,Avg_Open_To_Buy#32,Total_Amt_Chng_Q4_Q1#33,Total_Trans_Amt#34,Total_Trans_Ct#35,Total_Ct_Chng_Q4_Q1#36,Avg_Utilization_Ratio#37] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/keerthan/Learnings/Pyspark_Practice/Datasets/BankChurners...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<CLIENTNUM:int,Attrition_Flag:string,Customer_Age:int,Gender:string,Dependent_count:int,Edu...\n",
      "\n",
      "\n",
      "root\n",
      " |-- CLIENTNUM: integer (nullable = true)\n",
      " |-- Attrition_Flag: string (nullable = true)\n",
      " |-- Customer_Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Dependent_count: integer (nullable = true)\n",
      " |-- Education_Level: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- Income_Category: string (nullable = true)\n",
      " |-- Card_Category: string (nullable = true)\n",
      " |-- Months_on_book: integer (nullable = true)\n",
      " |-- Total_Relationship_Count: integer (nullable = true)\n",
      " |-- Months_Inactive_12_mon: integer (nullable = true)\n",
      " |-- Contacts_Count_12_mon: integer (nullable = true)\n",
      " |-- Credit_Limit: double (nullable = true)\n",
      " |-- Total_Revolving_Bal: integer (nullable = true)\n",
      " |-- Avg_Open_To_Buy: double (nullable = true)\n",
      " |-- Total_Amt_Chng_Q4_Q1: double (nullable = true)\n",
      " |-- Total_Trans_Amt: integer (nullable = true)\n",
      " |-- Total_Trans_Ct: integer (nullable = true)\n",
      " |-- Total_Ct_Chng_Q4_Q1: double (nullable = true)\n",
      " |-- Avg_Utilization_Ratio: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header=True, inferSchema=True).csv(file_path)\n",
    "df.explain()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91e20af-abbb-4abd-96ff-7a6d7cd262a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [plan_id=28]\n",
      "   +- FileScan csv [CLIENTNUM#17,Attrition_Flag#18,Customer_Age#19,Gender#20,Dependent_count#21,Education_Level#22,Marital_Status#23,Income_Category#24,Card_Category#25,Months_on_book#26,Total_Relationship_Count#27,Months_Inactive_12_mon#28,Contacts_Count_12_mon#29,Credit_Limit#30,Total_Revolving_Bal#31,Avg_Open_To_Buy#32,Total_Amt_Chng_Q4_Q1#33,Total_Trans_Amt#34,Total_Trans_Ct#35,Total_Ct_Chng_Q4_Q1#36,Avg_Utilization_Ratio#37] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/keerthan/Learnings/Pyspark_Practice/Datasets/BankChurners...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<CLIENTNUM:int,Attrition_Flag:string,Customer_Age:int,Gender:string,Dependent_count:int,Edu...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.repartition(4)\n",
    "df2.explain()\n",
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf0d8d8-8730-46f9-b474-598fa34effe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|partition_count|count|\n",
      "+---------------+-----+\n",
      "|              0| 2531|\n",
      "|              1| 2532|\n",
      "|              2| 2532|\n",
      "|              3| 2532|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn('partition_count', spark_partition_id()).groupBy('partition_count').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0836bd8b-65cc-4959-92ec-476f62397b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartitioning using the card category column (Assuming partition column is card_category)\n",
    "df3 = df.repartition(4, col('Card_Category'))\n",
    "df3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50f75bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0| 9436|\n",
      "|           2|  691|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.withColumn('partition_id', spark_partition_id()).groupBy('partition_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8c09ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|Card_Category|count|\n",
      "+-------------+-----+\n",
      "|         Blue| 9436|\n",
      "|         Gold|  116|\n",
      "|       Silver|  555|\n",
      "|     Platinum|   20|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy('Card_Category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf5c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
